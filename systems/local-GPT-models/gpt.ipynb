{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from pydantic import BaseModel\n",
    "url = open(\"local_models_path.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(272, 4) (790, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>dataset</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Welcome to KBismarck.org! This is a community ...</td>\n",
       "      <td>Information/Explanation</td>\n",
       "      <td>EN-GINCO</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why graft thrives in postconflict zones &lt;p&gt; A ...</td>\n",
       "      <td>News</td>\n",
       "      <td>EN-GINCO</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                   labels  \\\n",
       "0  Welcome to KBismarck.org! This is a community ...  Information/Explanation   \n",
       "1  Why graft thrives in postconflict zones <p> A ...                     News   \n",
       "\n",
       "    dataset language  \n",
       "0  EN-GINCO  English  \n",
       "1  EN-GINCO  English  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the test datasets from the GitHub repositories (access to them is obtained by request to the AGILE repository owner)\n",
    "\n",
    "en_ginco = pd.read_json(\"../../datasets/EN-GINCO-test-dataset/EN-GINCO.jsonl\", lines=True)\n",
    "x_ginco = pd.read_json(\"../../datasets/X-GINCO-test-set/X-GINCO.jsonl\", lines=True)\n",
    "\n",
    "print(en_ginco.shape, x_ginco.shape)\n",
    "\n",
    "en_ginco.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_local_model(model, prompt, url=url):\n",
    "\n",
    "\tclass ReponseStructure(BaseModel):\n",
    "\t\tgenre: int\n",
    "\n",
    "\tdata = {\n",
    "\t    \"model\": model,\n",
    "\t    \"prompt\": prompt,\n",
    "\t    \"stream\": False,\n",
    "\t    \"temperature\": 0,\n",
    "\t    \"format\": ReponseStructure.model_json_schema()\n",
    "\t}\n",
    "\n",
    "\theaders = {\"Content-Type\": \"application/json\",}\n",
    "\tresponse = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "\treturn response.json()[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gemma3:27b\", \"gemma2:27b\", \"deepseek-r1:14b\", \"llama3.3:latest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gpt(df_test_name, gpt_model):\n",
    "\n",
    "\tdfs = {\n",
    "\t\t\"en-ginco\": en_ginco,\n",
    "\t\t\"x-ginco\": x_ginco\n",
    "\t}\n",
    "\n",
    "\tdf = dfs[df_test_name]\n",
    "\n",
    "\tresponses = []\n",
    "\t\n",
    "\ttexts = df[\"text\"].to_list()\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tlabels_dict = {\n",
    "\t\t0: \"Other\",\n",
    "\t\t1: \"Information/Explanation\",\n",
    "\t\t2: \"News\",\n",
    "\t\t3: \"Instruction\",\n",
    "\t\t4: \"Opinion/Argumentation\",\n",
    "\t\t5: \"Forum\",\n",
    "\t\t6: \"Prose/Lyrical\",\n",
    "\t\t7: \"Legal\",\n",
    "\t\t8: \"Promotion\"\n",
    "\t}\n",
    "\n",
    "\tfor text in texts:\n",
    "\t\tcurrent_prompt = f\"\"\"\n",
    "\t\t\t\t### Task\n",
    "\t\t\t\tYour task is to classify the following text according to genre. Genres are text types, defined by the function of the text, authorâ€™s purpose and form of the text. Always provide a label, even if you are not sure.\n",
    "\n",
    "\t\t\t\t### Output format\n",
    "\t\t\t\t\tReturn a valid JSON dictionary with the following key: 'genre' and a value should be an integer which represents one of the labels according to the following dictionary: {labels_dict}.\n",
    "\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tText: '{text}'\n",
    "\t\t\t\"\"\"\n",
    "\n",
    "\t\tinitial_response= run_local_model(gpt_model, current_prompt, url=url)\n",
    "\n",
    "\t\tresponse = initial_response.replace(\"\\n\", \"\")\n",
    "\t\tresponse = response.replace(\"\\t\", \"\")\n",
    "\n",
    "\t\t# Convert the string into a dictionary\n",
    "\t\tresponse = json.loads(response)\n",
    "\n",
    "\t\t# Get out a label\n",
    "\t\ttry:\n",
    "\t\t\tpredicted = labels_dict[response[\"genre\"]]\n",
    "\t\t\tresponses.append(predicted)\n",
    "\t\t# add a possibility of something going wrong\n",
    "\t\texcept:\n",
    "\t\t\tpredicted = initial_response\n",
    "\t\t\tprint(\"error with extracting a label:\")\n",
    "\t\t\tprint(initial_response)\n",
    "\t\t\tresponses.append(predicted)\n",
    "\n",
    "\tend_time = time.time()\n",
    "\telapsed_time_min = end_time-start_time\n",
    "\n",
    "\tprint(f\"Prediction finished. It took {elapsed_time_min/60} min for {df.shape[0]} instances - {elapsed_time_min/df.shape[0]} s per instance.\")\n",
    "\n",
    "\t# Create a json with results\n",
    "\n",
    "\tif gpt_model == \"hf.co/tknez/GaMS-9B-Instruct-GGUF:latest\":\n",
    "\t\tgpt_model = \"GaMS-9B-Instruct\"\n",
    "\n",
    "\tcurrent_results = {\n",
    "\t\t\"system\": gpt_model,\n",
    "\t\t\"predictions\": [\n",
    "\t\t\t{\n",
    "\t\t\t\"train\": \"X-GENRE (train split)\",\n",
    "\t\t\t\"test\": \"{}\".format(df_test_name),\n",
    "\t\t\t\"predictions\": responses,\n",
    "\t\t\t}\n",
    "\t\t]\n",
    "\t\t}\n",
    "\n",
    "\t# Save the results as a new json\n",
    "\twith open(\"submissions/submission-{}-{}.json\".format(gpt_model, df_test_name), \"w\") as file:\n",
    "\t\tjson.dump(current_results, file)\n",
    "\n",
    "\tprint(\"Classification with {} on {} finished.\".format(gpt_model, df_test_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in [\"en-ginco\", \"x-ginco\"]:\n",
    "\tfor model in models:\n",
    "\t\tprint(model)\n",
    "\t\tpredict_gpt(test, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gemma3:27b', 'gemma2:27b', 'deepseek-r1:14b', 'llama3.3:latest']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning (ML) is a subset of artificial intelligence that focuses on the use of data and algorithms to enable computers to improve at tasks with experience. The key aspects of machine learning are:\n",
      "\n",
      "1. Data: Machine learning models rely on large amounts of data to learn patterns, make predictions, or decisions. This data can come from various sources such as text documents, images, audio files, sensor readings, etc.\n",
      "2. Algorithms: These algorithms are designed to process data and improve at a task through experience. They include supervised learning (where the data is labeled with desired outcomes), unsupervised learning (where the data is unlabeled and patterns are discovered), reinforcement learning (where an agent learns by taking actions in an environment).\n",
      "3. Experience: Through exposure to data, machine learning models gradually improve their performance on a task over time. For example, they might become better at classifying images of cats as more examples of cat pictures are shown to them.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "url = 'http://kt-gpu5.ijs.si:11435/api/generate'\n",
    "data = {\n",
    "    \"model\": \"hf.co/tknez/GaMS-9B-Instruct-GGUF:latest\",\n",
    "    \"prompt\": \"Explain what machine learning is.\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "print(result['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf.co/tknez/GaMS-9B-Instruct-GGUF:latest\n",
      "error with extracting a label:\n",
      "{\"genre\" : 2018}\n",
      "\n",
      "error with extracting a label:\n",
      "{\n",
      "\t\"genre\": 2567899071273546\n",
      "}\n",
      "\n",
      "Prediction finished. It took 1.8320292750994365 min for 272 instances - 0.40412410480134625 s per instance.\n",
      "Classification with GaMS-9B-Instruct on en-ginco finished.\n",
      "hf.co/tknez/GaMS-9B-Instruct-GGUF:latest\n",
      "Prediction finished. It took 5.4743201772371926 min for 790 instances - 0.41577115270155895 s per instance.\n",
      "Classification with GaMS-9B-Instruct on x-ginco finished.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate GaMS as well\n",
    "\n",
    "for test in [\"en-ginco\", \"x-ginco\"]:\n",
    "\tfor model in [\"hf.co/tknez/GaMS-9B-Instruct-GGUF:latest\"]:\n",
    "\t\tprint(model)\n",
    "\t\tpredict_gpt(test, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emma_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
