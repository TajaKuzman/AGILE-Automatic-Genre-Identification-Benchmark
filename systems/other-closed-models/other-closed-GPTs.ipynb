{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=open(\"API_key\", \"r\").read(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(272, 4) (790, 6)\n"
     ]
    }
   ],
   "source": [
    "# Load the test datasets from the GitHub repositories (access to them is obtained by request to the AGILE repository owner)\n",
    "\n",
    "en_ginco = pd.read_json(\"../../datasets/EN-GINCO-test-dataset/EN-GINCO.jsonl\", lines=True)\n",
    "x_ginco = pd.read_json(\"../../datasets/X-GINCO-test-set/X-GINCO.jsonl\", lines=True)\n",
    "\n",
    "print(en_ginco.shape, x_ginco.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gpt(df_test_name, gpt_model):\n",
    "\n",
    "\tdfs = {\n",
    "\t\t\"en-ginco\": en_ginco,\n",
    "\t\t\"x-ginco\": x_ginco\n",
    "\t}\n",
    "\n",
    "\tdf = dfs[df_test_name]\n",
    "\n",
    "\tresponses = []\n",
    "\t\n",
    "\ttexts = df[\"text\"].to_list()\n",
    "\n",
    "\tlabels_dict = {\n",
    "\t\t0: \"Other\",\n",
    "\t\t1: \"Information/Explanation\",\n",
    "\t\t2: \"News\",\n",
    "\t\t3: \"Instruction\",\n",
    "\t\t4: \"Opinion/Argumentation\",\n",
    "\t\t5: \"Forum\",\n",
    "\t\t6: \"Prose/Lyrical\",\n",
    "\t\t7: \"Legal\",\n",
    "\t\t8: \"Promotion\"\n",
    "\t}\n",
    "\n",
    "\tlabel_dict_with_description_ext = {\n",
    "\t\t\t\"Information/Explanation - An objective text that describes or presents an event, a person, a thing, a concept etc. Its main purpose is to inform the reader about something. Common features: objective/factual, explanation/definition of a concept (x is …), enumeration. E.g., research article, encyclopedia article, informational blog, product specification, course materials, general information, job description, manual, horoscope, travel guide, glossaries, historical article, biographical story/history.\": 1,\n",
    "\t\t\t\"News - An objective or subjective text which reports on an event recent at the time of writing or coming in the near future. Common features: adverbs/adverbial clauses of time and/or place (dates, places), many proper nouns, direct or reported speech, past tense. E.g., news report, sports report, travel blog, reportage, police report, announcement.\": 2,\n",
    "\t\t\t\"Instruction - An objective text which instructs the readers on how to do something. Common features: multiple steps/actions, chronological order, 1st person plural or 2nd person, modality (must, have to, need to, can, etc.), adverbial clauses of manner (in a way that), of condition (if), of time (after …). E.g., how-to texts, recipes, technical support.\": 3,\n",
    "\t\t\t\"Opinion/Argumentation - A subjective text in which the authors convey their opinion or narrate their experience. It includes promotion of an ideology and other non-commercial causes. This genre includes a subjective narration of a personal experience as well. Common features: adjectives/adverbs that convey opinion, words that convey (un)certainty (certainly, surely), 1st person, exclamation marks. E.g., review, blog (personal blog, travel blog), editorial, advice, letter to editor, persuasive article or essay, formal speech, pamphlet, political propaganda, columns, political manifesto.\": 4,\n",
    "\t\t\t\"Forum - A text in which people discuss a certain topic in form of comments. Common features: multiple authors, informal language, subjective (the writers express their opinions), written in 1st person. E.g., discussion forum, reader/viewer responses, QA forum.\": 5,\n",
    "\t\t\t\"Prose/Lyrical - A literary text that consists of paragraphs or verses. A literary text is deemed to have no other practical purpose than to give pleasure to the reader. Often the author pays attention to the aesthetic appearance of the text. It can be considered as art. E.g., lyrics, poem, prayer, joke, novel, short story. \": 6,\n",
    "\t\t\t\"Legal - An objective formal text that contains legal terms and is clearly structured. The name of the text type is often included in the headline (contract, rules, amendment, general terms and conditions, etc.). Common features: objective/factual, legal terms, 3rd person. E.g., small print, software license, proclamation, terms and conditions, contracts, law, copyright notices, university regulation.\": 7,\n",
    "\t\t\t\"Promotion - A subjective text intended to sell or promote an event, product, or service. It addresses the readers, often trying to convince them to participate in something or buy something. Common features: contains adjectives/adverbs that promote something (high-quality, perfect, amazing), comparative and superlative forms of adjectives and adverbs (the best, the greatest, the cheapest), addressing the reader (usage of 2nd person), exclamation marks. E.g., advertisement, promotion of a product (e-shops), promotion of an accommodation, promotion of company's services, invitation to an event.\": 8,\n",
    "\t\t\t\"Other - A text that which does not fall under any of other genre categories.\": 0,\n",
    "\n",
    "\t}\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\n",
    "\tfor text in texts:\n",
    "\t\tcompletion = client.chat.completions.create(model=gpt_model,\n",
    "\t\tresponse_format= {\"type\": \"json_object\"},\n",
    "\t\tmessages= [\n",
    "\t\t{\n",
    "\t\t\t\"role\": \"user\",\n",
    "\t\t\t\"content\": f\"\"\"\n",
    "\t\t\t### Task\n",
    "\t\t\tYour task is to classify the following text according to genre. Genres are text types, defined by the function of the text, author’s purpose and form of the text. Always provide a label, even if you are not sure.\n",
    "\n",
    "\t\t\t### Output format\n",
    "\t\t\t\tReturn a valid JSON dictionary with the following key: 'genre' and a value should be an integer which represents one of the labels according to the following dictionary: {label_dict_with_description_ext}.\n",
    "\n",
    "\t\t\t\t\n",
    "\t\t\t\tText: '{text}'\n",
    "\t\t\"\"\"\n",
    "\t\t\t}\n",
    "\t\t],\n",
    "\t\ttemperature = 0)\n",
    "\n",
    "\t\tresponse=completion.choices[0].message.content\n",
    "\n",
    "\t\tresponse = response.replace(\"\\n\", \"\")\n",
    "\t\tresponse = response.replace(\"\\t\", \"\")\n",
    "\n",
    "\t\t# Get out a label\n",
    "\t\ttry:\n",
    "\t\t\t# Convert the string into a dictionary\n",
    "\t\t\tresponse = json.loads(response)\n",
    "\t\t\tpredicted = labels_dict[response[\"genre\"]]\n",
    "\t\t\tresponses.append(predicted)\n",
    "\t\t# add a possibility of something going wrong\n",
    "\t\texcept:\n",
    "\t\t\tprint(\"error with extracting a label\")\n",
    "\t\t\tprint(response)\n",
    "\t\t\tresponses.append(response)\n",
    "\n",
    "\tend_time = time.time()\n",
    "\telapsed_time_min = end_time-start_time\n",
    "\n",
    "\tprint(f\"Prediction finished. It took {elapsed_time_min/60} min for {df.shape[0]} instances - {elapsed_time_min/df.shape[0]} s per instance.\")\n",
    "\n",
    "\t# Create a json with results\n",
    "\n",
    "\tcurrent_results = {\n",
    "\t\t\"system\": gpt_model,\n",
    "\t\t\"predictions\": [\n",
    "\t\t\t{\n",
    "\t\t\t\"train\": \"NA (zero-shot)\",\n",
    "\t\t\t\"test\": \"{}\".format(df_test_name),\n",
    "\t\t\t\"predictions\": responses,\n",
    "\t\t\t}\n",
    "\t\t]\n",
    "\t\t}\n",
    "\n",
    "\t# The only thing that needs to be changed in the code from OpenAI\n",
    "\tgpt_model_name = gpt_model.split(\"/\")[1]\n",
    "\n",
    "\t# Save the results as a new json\n",
    "\twith open(\"submissions/submission-{}-{}.json\".format(gpt_model_name, df_test_name), \"w\") as file:\n",
    "\t\tjson.dump(current_results, file)\n",
    "\n",
    "\tprint(\"Classification with {} on {} finished.\".format(gpt_model_name, df_test_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"google/gemini-2.5-flash\", \"mistralai/mistral-medium-3.1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/mistral-medium-3.1\n",
      "Prediction finished. It took 3.6682628830273947 min for 272 instances - 0.8091756359619253 s per instance.\n",
      "Classification with mistral-medium-3.1 on en-ginco finished.\n"
     ]
    }
   ],
   "source": [
    "# Continue with prediction\n",
    "for test in [\"en-ginco\"]:\n",
    "\tfor model in [\"mistralai/mistral-medium-3.1\"]:\n",
    "\t\tprint(model)\n",
    "\t\tpredict_gpt(test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/gemini-2.5-flash\n",
      "Prediction finished. It took 6.5814470489819845 min for 790 instances - 0.4998567378973659 s per instance.\n",
      "Classification with gemini-2.5-flash on x-ginco finished.\n",
      "mistralai/mistral-medium-3.1\n",
      "Prediction finished. It took 7.830524067083995 min for 790 instances - 0.5947233468671388 s per instance.\n",
      "Classification with mistral-medium-3.1 on x-ginco finished.\n"
     ]
    }
   ],
   "source": [
    "for test in [\"x-ginco\"]:\n",
    "\tfor model in models:\n",
    "\t\tprint(model)\n",
    "\t\tpredict_gpt(test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/gemini-2.5-pro\n",
      "Prediction finished. It took 46.625453011194864 min for 272 instances - 10.28502639952828 s per instance.\n",
      "Classification with gemini-2.5-pro on en-ginco finished.\n",
      "google/gemini-2.5-pro\n",
      "error with extracting a label\n",
      "\n",
      "error with extracting a label\n",
      "\n",
      "error with extracting a label\n",
      "\n",
      "error with extracting a label\n",
      "\n",
      "error with extracting a label\n",
      "\n",
      "Prediction finished. It took 148.74551327228545 min for 790 instances - 11.297127590300162 s per instance.\n",
      "Classification with gemini-2.5-pro on x-ginco finished.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate also Gemini 2.5 Pro\n",
    "for test in [\"en-ginco\", \"x-ginco\"]:\n",
    "\tmodel = \"google/gemini-2.5-pro\"\n",
    "\tprint(model)\n",
    "\tpredict_gpt(test, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emma_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
